{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Visual Exploratory\n",
    "#Bar plots are for discrete data counts, great for multiple variables, especially when one is categorical\n",
    "#Histograms are for continuous data counts, great for single variables\n",
    "#Scatter plots display relationshop between two numberic variables\n",
    "#Columns are variables, rows are observations\n",
    "\n",
    "#Inspecting\n",
    ".head()\n",
    ".columns\n",
    ".info()\n",
    ".describe() \n",
    ".dtypes\n",
    ".tail()\n",
    ".shape\n",
    "\n",
    "df['column'].unique() #Prints unique values\n",
    "df['column'].nunique() #Returns the number of unique values\n",
    "df[\"column\"].value_counts(dropna = False) #Totals frequency of each unique value, sorted by highest frequency\n",
    "df.idmax #(axis='columns')\n",
    "df.idmin #(axis='columns') \n",
    "df.isin([])\n",
    "            \n",
    "#Assigning column header\n",
    "df.columns = ['col1', 'col2']\n",
    "\n",
    "#Assigning index\n",
    "pd.read_csv(index_col='col')\n",
    "df.index = ['column_name']\n",
    "df.index.name = 'new_name'\n",
    "df.set_index('col', inplace=True)\n",
    "df.reset_index()['column']\n",
    "df.sort_index()\n",
    "df.reindex()#index values are immutable, but can change out all values at once\n",
    "\n",
    "#Sorting\n",
    "df.sort_values('column', ascending=False)\n",
    "\n",
    "#Changing types\n",
    "df.column.astype(str)\n",
    "df.column.astype('category')\n",
    "pd.to_numeric(df['column'], errors='coerce') #int64, #coerce will turn blank values into NaN\n",
    "df.values #converts df to numpy array\n",
    "df.column = pd.Categorical(values=df.column, categories=['Bronze', 'Silver', 'Gold'], ordered=True)\n",
    "\n",
    "#Duplicate and Missing Data\n",
    "df.drop_duplicates()\n",
    "df.dropna(how='any') # for rows #how='all' #thresh=1000\n",
    "df.drop(list_to_drop, axis='columns')\n",
    "  \n",
    "#Filtering 0's and NaN's    \n",
    "df.all() #all nonzero...filters columns with any '0'\n",
    "df.any() #any nonzeros...filters columns with all 0's\n",
    "\n",
    "df.isnull.all() #Filters columns with any NaNs\n",
    "df.isnull.any() #Filters columns with all NaNs\n",
    "            \n",
    "df.notnull.all() \n",
    "df.notnull.any()\n",
    "\n",
    "#Filling NaN\n",
    "df.column = df.column.fillna('value or function')\n",
    "df.column = df.column.ffill('value')\n",
    "\n",
    "#Checking for NaN values w/ asserts\n",
    "assert pd.notnull(df).all().all()\n",
    "            \n",
    "#Filtering and aggregation      \n",
    "df.groupby(['Columns to group by', 'second column to group by'])  \n",
    "df.groupby(df2['column']) #then try df['column'].mean()\n",
    "df.groupby('column')['Column to select']\n",
    "df.groupby('column')[['Column to select', 'Column to select']].agg(['max', 'sum'])\n",
    "#Custom aggregations via dictinaries\n",
    "df.groupby('column')[['col1', 'Col2']].agg({'col1':'max', 'col2':'sum'})  \n",
    "#or\n",
    "sets[['year', 'theme_id']].groupby('year', as_index = False).agg({\"theme_id\": pd.Series.nunique})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Manipulating DataFrames\n",
    "#Melting: turns columns into rows\n",
    "pd.melt(frame = df, id_vars = 'fixed cols', value_vars = 'cols to melt', \n",
    "        var_name = 'new col name', value_name = 'new value column name')\n",
    "\n",
    "pd.melt(df, col_level=0) #returns column at index 0 was key-value pairs\n",
    "\n",
    "#Pivoting, turning unique values from a column into new columns\n",
    "df.pivot(index = 'col', columns = 'col', values = 'columns to use as values')\n",
    "\n",
    "#Pivot Table...Pivot will err when finding duplicate indices, so we use pivot table instead, creates hia index\n",
    "df.pivot_table(index = 'col', columns = 'col', values = 'col', aggfunc = 'how to deal with dups', margins=True)\n",
    "\n",
    "#Appending: stacks vertically\n",
    "df1.append(df2) \n",
    "\n",
    "#Concatenating: stacks verticaly or horizontally on the index, supports inner and outer joins\n",
    "pd.concat([df1, df2], ignore_index = True, axis=0, keys=[], join='inner or outer')\n",
    "\n",
    "#Join: joins on index, supports inner, outer, right and left\n",
    "df.join(how='left, right, inner, or outer') #joins on the index\n",
    "\n",
    "#Merging: Data merges on a specific column of group of colums, supports many joins\n",
    "pd.merge(left=df_1, right=df_1, left_on='df_1_column', right_on='df_2_column')\n",
    "pd.merge(df, df2, on=['col1', 'col2', 'col3'], how='inner, outer, right or left')\n",
    "\n",
    "#Merge and order\n",
    "pd.merge_ordered('same para as above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Concatenating many dataframes by using glob\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "#Create pattern\n",
    "pattern = '*.csv'\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Create an empty list: frames\n",
    "frames = []\n",
    "\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.DataFrame(pd.read_csv(csv))\n",
    "    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "big_frame = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#String Parsing\n",
    "#Parsing strings in columns\n",
    "df['name'] = df.column_name.str[0]\n",
    "\n",
    "#Splitting column \n",
    "    ##Step 1. Split column...this creates a list in the column\n",
    "df['str_split'] = df['type_and_country'].str.split('_')#.get(0) ....one line\n",
    "\n",
    "    ##Step2. Create new column from indexing the list that is created from the split\n",
    "df['type'] = df.str_split.str.get(0)\n",
    "df['country'] = df.str_split.str.get(1)\n",
    "\n",
    "\n",
    "#Pattern Matching\n",
    "# Import the regular expression module\n",
    "import re\n",
    "from numpy import NaN\n",
    "\n",
    "# Compile the pattern: pattern\n",
    "pattern = re.compile('\\d{3}-\\d{3}-\\d{4}')\n",
    "\n",
    "# See if the pattern matches\n",
    "result = prog.match('123-456-7890')\n",
    "print(bool(result))\n",
    "\n",
    "# See if the pattern matches\n",
    "result = prog.match('1123-456-7890')\n",
    "print(bool(result))\n",
    "\n",
    "#on one line\n",
    "(re.compile(\"\\d{3}-\\d{3}-\\d{4}\")(x))\n",
    "\n",
    "\n",
    "#Finding numberic values\n",
    "matches = re.findall('\\d+', 'the recipe calls for 10 strawberries and 1 banana')\n",
    "\n",
    "# Print the matches\n",
    "print(matches)\n",
    "\n",
    "#A few more patterns\n",
    "# Write the second pattern\n",
    "pattern2 = bool(re.match(pattern='\\$\\d*\\.\\d{2}', string='$123.45'))\n",
    "print(pattern2)\n",
    "\n",
    "# Write the third pattern\n",
    "pattern3 = bool(re.match(pattern='[A-Z]\\w*', string='Australia'))\n",
    "print(pattern3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Using Functions to Clean Data\n",
    "# Define func\n",
    "def recode_sex(sex_value):\n",
    "\n",
    "    # Return 1 if sex_value is 'Male'\n",
    "    if sex_value == 'Male':\n",
    "        return 1\n",
    "    \n",
    "    # Return 0 if sex_value is 'Female'    \n",
    "    elif sex_value == 'Female':\n",
    "        return 0\n",
    "    \n",
    "    # Return np.nan    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the sex column using .apply\n",
    "df['sex_recode'] = df.sex_column.apply(recode_sex, axis=0) #no need to add () after the funciton\n",
    "\n",
    "#Using Lambdas to Clean Data\n",
    "# Write the lambda function using replace\n",
    "df['total_dollar_replace'] = df.total_dollar.apply(lambda x: x.replace('$', ''))\n",
    "\n",
    "# Write the lambda function using regular expressions\n",
    "df['total_dollar_re'] = df.total_dollar.apply(lambda x: re.findall('\\d+\\.\\d+', x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Arithmatic\n",
    "df.divide(df2, axis='rows')\n",
    "df.pct_change() * 100\n",
    "df.add(df2)\n",
    "df.add(df2, fill_value=0)\n",
    "df.multiply(df2, axis='rows')\n",
    ".mean(axis='columns') #This computes the mean of all columns per row.\n",
    "df.quantile([0.05, 0.95]#the 5th and 95th percentiles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
