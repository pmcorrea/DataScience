{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pyspark RDD's and pyspark DataFrames are immutable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "# import pyspark\n",
    "# # import random\n",
    "# sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "# # num_samples = 1000\n",
    "# # def inside(p):     \n",
    "# #   x, y = random.random(), random.random()\n",
    "# #   return x*x + y*y < 1\n",
    "# # count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "# # pi = 4 * count / num_samples\n",
    "# # print(pi)\n",
    "# print(sc.version)\n",
    "# print(sc.version)\n",
    "# print(sc.pythonVer)\n",
    "# print(sc.localr)\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkContext"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sc.version\n",
    "sc.pythonVer\n",
    "sc.localr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD's"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "sc.paralellize([someList]) #arg must be a list\n",
    "sc.textFile('someText.txt')\n",
    "spark.read.json('example.json')\n",
    "spark.read.csv('example.csv')\n",
    "spark.read.paquet('example.parq')\n",
    "\n",
    "# Transformations\n",
    "#Return new RDD\n",
    "rdd.map()\n",
    "rdd.filter()\n",
    "rdd.flatMap()\n",
    "rdd.union()\n",
    "\n",
    "\n",
    "# Actions\n",
    "# Perform computation on RDD\n",
    "rdd.collect()\n",
    "rdd.take()\n",
    "rdd.first()\n",
    "rdd.count()\n",
    "rdd.reduce(func)\n",
    "rdd.saveAsTextFile() #Saves each partitions as a seperate file\n",
    "rdd.coalesce().saveAsTextFile() #Saves all partitions into a single file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair RDD's"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# RDD's in key-value form\n",
    "\n",
    "#EXAMPLE_1\n",
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
    "pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "\n",
    "#EXAMPLE_2\n",
    "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\n",
    "regularRDD = sc.parallelize(my_list)\n",
    "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\n",
    "\n",
    "#Transformations\n",
    "pair_rdd.reduceByKey()\n",
    "pair_rdd.groupByKey()\n",
    "pair_rdd.sortByKey()\n",
    "pair_rdd.join()\n",
    "\n",
    "#Actions\n",
    "pair_rdd.countByKey()\n",
    "pair_rdd.collectAsMap() #Returns k/v pairs as dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark SQL, DataFrames, and SparkSession\n",
    "### PySparkSQL works on DataFrames not RDD's"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Creating DataFrame\n",
    "spark.createDataFrame(rdd, schema=list)\n",
    "spark.read.csv(file_path, header=True, schema=infer)\n",
    "spark.read.json(file_path, header=True, schema=infer)\n",
    "spark.read.txt(file_path, header=True, schema=infer)\n",
    "\n",
    "# Exploring The Dataframe\n",
    "df.count()      #counts rows\n",
    "len(df.columns) #counts columns\n",
    "\n",
    "# Descriptive with DataFrames\n",
    "df.dtypes       #prints a list of columns with dtypes\n",
    "pyspark.sql.functions.mean(col)\n",
    "pyspark.sql.functions.skewness(col)\n",
    "pyspark.sql.functions.min(col)\n",
    "df.cov(col1, col2)\n",
    "df.corr(col1, col2)\n",
    "\n",
    "# Aggregating Data\n",
    "df.agg({'COLUMN':'max'}).collect()\n",
    "\n",
    "\n",
    "# DataFrame API\n",
    "# Transformations\n",
    "df.select()\n",
    "df.filter()\n",
    "df.groupBy()\n",
    "df.orderBy()\n",
    "df.dropDuplicates()\n",
    "df.withColumnRenamed()\n",
    "df.withColumn()\n",
    "\n",
    "# DataFrame API\n",
    "# Actions\n",
    "df.printSchema()\n",
    "df.show()\n",
    "df.count()\n",
    "df.columns()\n",
    "df.describe()\n",
    "\n",
    "# SQL Queries\n",
    "# 1. Create TempView, \n",
    "# 2.Run query using spark.sql() and assign it to a new variable\n",
    "df.createOrReplaceTempView(\"table1\")\n",
    "df2 = spark.sql(\"SELECT field1, field2 FROM table1\")\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Wrangling, Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Values\n",
    "df = df.dropna()  # Drop any records with NULL values\n",
    "\n",
    "# drop records if both LISTPRICE and SALESCLOSEPRICE are NULL\n",
    "# any = if any values are null, all = only if all values are null\n",
    "df = df.dropna(how='all', subset['LISTPRICE', 'SALESCLOSEPRICE '])\n",
    "df = df.dropna(thresh=2) # Drop records where at least two columns have NULL values\n",
    "df.dropDuplicates()\n",
    "df.dropDuplicates(['streetaddress']) # Check only a column list\n",
    "\n",
    "cols_to_drop = ['STREETNUMBERNUMERIC', 'LOTSIZEDIMENSIONS']\n",
    "df = df.drop(*cols_to_drop) # Drop columns from list using '*'\n",
    "\n",
    "# Text Filtering\n",
    "df = df.where(~df['column'].like('some text or value')) # ~ the tilde is the NOT condition \n",
    "\n",
    "# Scaling\n",
    "# (original value - min) / (max - min)\n",
    "\n",
    "# Standardization\n",
    "# (original value - mean) / (stddev)\n",
    "\n",
    "# Log Scaling\n",
    "from pyspark.sql.functions import log\n",
    "df.withColumn('new_column', log(df['old_column']))\n",
    "\n",
    "# Imputation\n",
    "df.fillna(valueToFillWith, list_of_columns)\n",
    "\n",
    "# PySpark DataFrame Join\n",
    "df.join(other_df, on='column with keys to join', how='left')\n",
    "\n",
    "# Date objects\n",
    "from pyspark.sql.functions import to_date, year, month, day, dayofmonth, weekofyear, datediff\n",
    "df = df.withColumn('new_col', to_date('col2'))\n",
    "df = df.withColumn('new_col', dayofmonth('col2'))\n",
    "df = df.withColumn('new_col', datediff('col2', 'col3'))\n",
    "\n",
    "# Lagging Features\n",
    "from pyspark.sql.functions import to_date, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().orderBy(df['col']) #create window \n",
    "lag_col = df.withColumn('new_col', lag('Col_to_lag_from', count=1).over(w))\n",
    "df.show()\n",
    "\n",
    "# Text Extraction \n",
    "greater_bool = df['col'].like('%Over Eight Years%')\n",
    "less_bool = df['col'].like('%Less Eight Years%')\n",
    "\n",
    "df = df.withColumn('new_col', (when(greater_bool, 1).when(less_bool, 0).otherwise(None)))\n",
    "\n",
    "# Split, Pivot and Explode\n",
    "split_col = split(df['col'], ',')\n",
    "new_col = df.withColumn('someCol', split_col.getItem(0))\n",
    "\n",
    "from pyspark.sql.functions import split, explode, lit, coalesce, first\n",
    "df = df.withColumn('new_col', split(df['col'], ','))\n",
    "ex_df = df.withColumn('ex_col', explode(df['new_col']))\n",
    "ex_df = df.withColumn('constant_val', lit(1))\n",
    "piv_df = df.groupBy('NO').pivot('ex_col').agg(coalesce(first('constant_val')))\n",
    "\n",
    "# Binarizing, Bucketing and Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Visualization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Spark lacks string visualization tools, as such one must sample and then export data to a pandas DataFrame for utilization.\n",
    "\n",
    "# Sampling Data\n",
    "df.sample(withReplacement=False, fraction=0.8, seed=42)\n",
    "df.toPandas()\n",
    "\n",
    "#pyspark_dist_explore library\n",
    "new_df = old_df.toPandas()\n",
    "\n",
    "df.hist(df, args)\n",
    "df.distplot()\n",
    "df.pandas_histogram()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Various tools provided by MLlib include:\n",
    "-ML Algorithms: collaborative filtering, classification, and clustering\n",
    "-Featurization: feature extraction, transformation, dimensionality reduction, and selection\n",
    "-Pipelines: tools for constructing, evaluating, and tuning ML Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing Sets\n",
    "data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "training, test=data.randomSplit([0.6, 0.4])\n",
    "training.collect()\n",
    "test.collect()\n",
    "\n",
    "## Collaborative Filtering\n",
    "## The MSE is the average value of the square of (actual rating - predicted rating)\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "r = Rating(user = 1, product = 2, rating = 5.0) #Rating class is a wrapper around tuple (user, product and rating)...(r[0], r[1], r[2])\n",
    "\n",
    "model = ALS.train(ratings, rank, iterations) #Rank is the number of features\n",
    "predictions = model.predictAll(unrated_RDD)\n",
    "predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PySpark MLlib contains specific data types Vectors and LabelledPoint.\n",
    "\n",
    "Two types of Vectors\n",
    "-Dense Vector: store all their entries in an array of floating point numbers\n",
    "-Sparse Vector: store only the nonzero values and their indices.\n",
    "    \n",
    "denseVec = Vectors.dense([1.0, 2.0, 3.0])\n",
    "DenseVector([1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A Labeledpoint is a wrapper for predicted value and input features.\n",
    "positive = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n",
    "negative = LabeledPoint(0.0, [2.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HashingTF() algorithm is used to map feature value to indices in the feature vector.\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "sentence = \"hello hello world\"\n",
    "words = sentence.split()\n",
    "tf = HashingTF(10000) \n",
    "tf.transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "data = [\n",
    "        LabeledPoint(0.0, [0.0, 1.0]),\n",
    "        LabeledPoint(1.0, [1.0, 0.0]),\n",
    "]\n",
    "RDD = sc.parallelize(data)\n",
    "model = LogisticRegressionWithLBFGS.train(RDD)\n",
    "lrm.predict([1.0, 0.0])\n",
    "lrm.predict([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "RDD = sc.textFile(\"WineData.csv\"). \\\n",
    "       map(lambda x: x.split(\",\")).\\\n",
    "       map(lambda x: [float(x[0]), float(x[1])])\n",
    "RDD.take(5)\n",
    "\n",
    "model = KMeans.train(RDD, k = 2, maxIterations = 10)\n",
    "model.clusterCenters\n",
    "\n",
    "from math import sqrt\n",
    "def error(point):\n",
    "    center = model.centers[model.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "WSSSE = RDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "wine_data_df = spark.createDataFrame(RDD, schema=[\"col1\", \"col2\"])\n",
    "wine_data_df_pandas = wine_data_df.toPandas()\n",
    "\n",
    "cluster_centers_pandas = pd.DataFrame(model.clusterCenters, columns=[\"col1\", \"col2\"])\n",
    "cluster_centers_pandas.head()\n",
    "\n",
    "plt.scatter(wine_data_df_pandas[\"col1\"], wine_data_df_pandas[\"col2\"]\n",
    "plt.scatter(cluster_centers_pandas[\"col1\"], cluster_centers_pandas[\"col2\"], color=\"red\", marker=\"x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
